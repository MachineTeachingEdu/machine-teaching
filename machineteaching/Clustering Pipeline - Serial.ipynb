{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB\n",
    "from questions.models import Solution, Cluster\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from analyzer import python_analyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from vectorizer import NCutVectorizer\n",
    "\n",
    "# Learning\n",
    "from clustering import Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems to be ignored: 591\n",
      "Problems to be used: 132\n",
      "Solutions to be used: 54\n",
      "Got 54 documents\n"
     ]
    }
   ],
   "source": [
    "## Cleaning database\n",
    "last_id = 132\n",
    "problems = Problem.objects.filter(id__gt=last_id)\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems).update(ignore=True)\n",
    "print(\"Problems to be ignored: %d\" % problems.count())\n",
    "\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "# problems = Problem.objects.all()\n",
    "print(\"Problems to be used: %d\" % problems.count())\n",
    "\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems, ignore=False).order_by('id')\n",
    "# solutions_obj = Solution.objects.all().order_by('id')\n",
    "print(\"Solutions to be used: %d\" % solutions_obj.count())\n",
    "\n",
    "docs_id = []\n",
    "questions = []\n",
    "solutions = []\n",
    "\n",
    "# Fill separated structures\n",
    "for sol in solutions_obj:\n",
    "    docs_id.append(sol.id)\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing solutions into bag of words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_integrity(doc):\n",
    "    file = io.StringIO(doc)\n",
    "    try:\n",
    "        for token in tokenize.generate_tokens(file.readline):\n",
    "            continue\n",
    "    except Exception:\n",
    "        return False\n",
    "    return True\n",
    "        \n",
    "def create_bag_of_words(docs, vectorizer_method, binary=False, min_df=0.2):\n",
    "    for idx, d in enumerate(docs):\n",
    "        if not test_integrity(d):\n",
    "            print(\"error on %d\" % idx)\n",
    "            \n",
    "    print(\"integrity ok\")\n",
    "            \n",
    "    vectorizer = vectorizer_method(analyzer = python_analyzer,\n",
    "                                   binary=binary,\n",
    "                                   min_df=min_df) \n",
    "    train_data_features = vectorizer.fit_transform(docs)\n",
    "    try:\n",
    "        train_data_features = train_data_features.toarray()\n",
    "    # It's already an array\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load coherence.py\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "\n",
    "def calculate_umass_coherence(X, word_topic, clusters, k, N=5):\n",
    "    k_coherence = []\n",
    "    for idx_cluster in range(k):\n",
    "        count_data = X.copy()\n",
    "        count_data[np.where(count_data != 0)] = 1\n",
    "        cluster_data = count_data[clusters == idx_cluster]\n",
    "\n",
    "        # If there aren't any documents assigned to the cluster, skip it\n",
    "        if cluster_data.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate cooccurence matrix\n",
    "        cluster_data[np.where(cluster_data > 1)] = 1\n",
    "        cooccurence = np.dot(cluster_data.T, cluster_data)\n",
    "\n",
    "        # For each topic, get N top words\n",
    "        idx = word_topic[:,idx_cluster].argsort()[::-1][:N]\n",
    "        perms = permutations(idx, 2)\n",
    "        k_score = []\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for i,j in perms:\n",
    "            if cooccurence[i,i] == 0:\n",
    "                continue\n",
    "            score = np.log((cooccurence[i,j]+0.01)/cooccurence[i,i])\n",
    "            k_score.append(score)\n",
    "        k_topic = np.mean(np.asarray(k_score))\n",
    "        k_coherence.append(k_topic)\n",
    "    return k_coherence, np.median(k_coherence), np.std(k_coherence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clusters(*args):\n",
    "    # Import libraries\n",
    "    # DB\n",
    "    import psycopg2\n",
    "    # Helpers\n",
    "    import base64\n",
    "    import pickle\n",
    "    import time\n",
    "    import numpy as np\n",
    "    # Learning\n",
    "    from clustering import Clustering\n",
    "    # Evaluation\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "    from gap import Gap\n",
    "#     from coherence import calculate_umass_coherence\n",
    "    \n",
    "    # Get arguments\n",
    "    dataset, v, m, b, train_data_features, k, dist, method = args\n",
    "    \n",
    "    # Instanciate objects\n",
    "    clustering = Clustering(train_data_features, k, metric=dist)\n",
    "    gap = Gap(train_data_features, k, nrefs=20, distance=dist)\n",
    "    \n",
    "    # Cluster\n",
    "    start = time.time()\n",
    "    model, document_topic, word_topic = getattr(clustering, method)()\n",
    "    clusters = document_topic.argmax(axis=1)\n",
    "    end = time.time()\n",
    "    clustering_time = end-start\n",
    "\n",
    "    # Compute Gap\n",
    "    start = time.time()\n",
    "    k_gap = gap.calculate_gap(clustering, method)\n",
    "#     time.sleep(10)\n",
    "#     k_gap = [3,2]\n",
    "    end = time.time()\n",
    "    gap_time = end-start\n",
    "\n",
    "    # Compute silhouette. Keep single to values to be able to plot it later\n",
    "    start = time.time()\n",
    "    k_silhouette = silhouette_score(train_data_features, clusters, metric=dist)\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(train_data_features, clusters, metric=dist)\n",
    "    end = time.time()\n",
    "    silhouette_time = end-start\n",
    "\n",
    "    # UMass coherence\n",
    "    start = time.time()\n",
    "    k_coherence = calculate_umass_coherence(train_data_features, word_topic, clusters, k)\n",
    "    end = time.time()\n",
    "    coherence_time = end-start\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": dataset,\n",
    "        \"X\": train_data_features,\n",
    "        \"y\": clusters,\n",
    "        \"vectorizer\": v.__name__,\n",
    "        \"is_binary\": b,\n",
    "        \"min_df\": m,\n",
    "        \"distance\": dist,\n",
    "        \"k\": k,\n",
    "        \"method\": method,\n",
    "#         \"model\": base64.b64encode(pickle.dumps(model)),\n",
    "        \"clustering_time\": clustering_time,\n",
    "#         \"gap\": k_gap[0],\n",
    "#         \"gap_std\": k_gap[1],\n",
    "#         \"gap_time\": gap_time,\n",
    "#         \"silhouette\": k_silhouette,\n",
    "#         \"silhouette_samples\": sample_silhouette_values,\n",
    "#         \"silhouette_time\": silhouette_time,\n",
    "        \"coherence_samples\": k_coherence[0],\n",
    "        \"coherence_med\": k_coherence[1],\n",
    "        \"coherence_std\": k_coherence[2],\n",
    "        \"coherence_time\": coherence_time,\n",
    "        \"coherence_k\": len(k_coherence[0]),\n",
    "    }\n",
    "    \n",
    "#     # Connect to DB\n",
    "#     connection = psycopg2.connect(user = \"machineteaching\",\n",
    "#                                   password = \"***REMOVED***\",\n",
    "#                                   host = \"localhost\",\n",
    "# #                                   port = \"5432\",\n",
    "#                                   database = \"machineteaching\")\n",
    "#     connection.autocommit=True\n",
    "#     cursor = connection.cursor()\n",
    "\n",
    "#     # Write PSQL query\n",
    "#     insert_query_base = \"INSERT INTO EXPERIMENTS \"\n",
    "#     column_value = []\n",
    "#     insert_format = []\n",
    "#     query_values = []\n",
    "#     for col in row.keys():\n",
    "#         if isinstance(row[col], np.ndarray):\n",
    "#             query_values.append(row[col].tolist())\n",
    "#         else:\n",
    "#             query_values.append(row[col])\n",
    "#         column_value.append(col)\n",
    "#         insert_format.append(\"%s\")\n",
    "\n",
    "#     insert_query = insert_query_base + \"(\" + \", \".join(column_value) + \") VALUES \"\n",
    "#     insert_query += \"(\" + \", \".join(insert_format) + \")\"\n",
    "#     query_values = tuple(query_values)\n",
    "#     query = cursor.mogrify(insert_query, query_values)\n",
    "#     cursor.execute(query)\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='vectorizer', max=1, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='min_df', max=1, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='binary', max=1, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integrity ok\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='clusters', max=1, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='metric', max=1, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='method', max=1, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.42 s, sys: 7.73 s, total: 10.2 s\n",
      "Wall time: 1.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmoraes/miniconda3/envs/machine_teaching/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/lmoraes/miniconda3/envs/machine_teaching/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/lmoraes/miniconda3/envs/machine_teaching/lib/python3.6/site-packages/numpy/lib/function_base.py:3250: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers = [\n",
    "#     CountVectorizer,\n",
    "    TfidfVectorizer, \n",
    "#     NCutVectorizer\n",
    "]\n",
    "# min_df = np.arange(0.05, 0.5, 0.05)\n",
    "min_df = [0.05]\n",
    "binary = [\n",
    "#     True, \n",
    "    False\n",
    "]\n",
    "cluster_methods = [\n",
    "    'nmf',\n",
    "#     'lda',\n",
    "]\n",
    "#, 'hierarchical', 'gaussian_mixture', 'spectral_clustering']\n",
    "metric = [\n",
    "#     'euclidean', \n",
    "    'cosine', \n",
    "#     'correlation'\n",
    "]\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Grid search\n",
    "for v in tqdm_notebook(vectorizers, desc=\"vectorizer\", leave=False):\n",
    "    for m in tqdm_notebook(min_df, desc=\"min_df\", leave=False):\n",
    "\n",
    "        for b in tqdm_notebook(binary, desc=\"binary\", leave=False):\n",
    "            train_data_features = create_bag_of_words(solutions, v, binary=b, min_df=m)\n",
    "\n",
    "            # Remove rows containing only zeros (weird exercises)\n",
    "            solution_sample = train_data_features[~(train_data_features==0).all(1)]\n",
    "            if solution_sample.shape != train_data_features.shape:\n",
    "                error = {\n",
    "                    \"vectorizer\": v,\n",
    "                    \"min_df\": m,\n",
    "                    \"binary\": b\n",
    "                }\n",
    "                print(\"ERROR: %s\" % error)\n",
    "\n",
    "#             clusters = range(2, int(np.sqrt(min(train_data_features.shape)))+1)\n",
    "            clusters = [3]\n",
    "            for k in tqdm_notebook(clusters, desc=\"clusters\", leave=False):\n",
    "                for dist in tqdm_notebook(metric, desc=\"metric\", leave=False):\n",
    "                    for method in tqdm_notebook(cluster_methods, desc=\"method\", leave=False):\n",
    "                        # Sequences to be sent to map function\n",
    "                        args = ['solution_all', v, m, b, train_data_features, \n",
    "                                k, dist, method]\n",
    "                        total += 1\n",
    "                        row = run_clusters(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'solution_all',\n",
       " 'X': array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.57220801,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " 'y': array([1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 2, 0, 2, 1]),\n",
       " 'vectorizer': 'TfidfVectorizer',\n",
       " 'is_binary': False,\n",
       " 'min_df': 0.05,\n",
       " 'distance': 'cosine',\n",
       " 'k': 3,\n",
       " 'method': 'nmf',\n",
       " 'clustering_time': 0.02829909324645996,\n",
       " 'coherence_samples': [-0.22275299244752336, -0.07205059608478748, nan],\n",
       " 'coherence_med': nan,\n",
       " 'coherence_std': nan,\n",
       " 'coherence_time': 0.0038726329803466797,\n",
       " 'coherence_k': 3}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
