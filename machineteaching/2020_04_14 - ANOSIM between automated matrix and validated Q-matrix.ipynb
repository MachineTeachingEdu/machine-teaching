{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import base64\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "# DB \n",
    "import psycopg2\n",
    "from django.conf import settings\n",
    "\n",
    "# Learning\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from skbio.stats.distance import anosim\n",
    "from skbio import DistanceMatrix\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import analyzer\n",
    "from tokenizer import create_bag_of_words\n",
    "from vectorizer import NCutVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get solutions from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems to be used: 132\n",
      "Solutions to be used: 54\n",
      "Got 54 documents\n"
     ]
    }
   ],
   "source": [
    "## Cleaning database\n",
    "last_id = 132\n",
    "# problems = Problem.objects.filter(id__gt=last_id)\n",
    "# # solutions_obj = Solution.objects.filter(problem__in=problems).update(ignore=True)\n",
    "# print(\"Problems to be ignored: %d\" % problems.count())\n",
    "\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "# problems = Problem.objects.all()\n",
    "print(\"Problems to be used: %d\" % problems.count())\n",
    "\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems, ignore=False).order_by('id')\n",
    "# solutions_obj = Solution.objects.all().order_by('id')\n",
    "print(\"Solutions to be used: %d\" % solutions_obj.count())\n",
    "\n",
    "docs_id = []\n",
    "questions = []\n",
    "solutions = []\n",
    "clusters = []\n",
    "\n",
    "# Fill separated structures\n",
    "for sol in solutions_obj:\n",
    "    docs_id.append(sol.id)\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "    clusters.append(sol.cluster.id)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(user = settings.DATABASES[\"default\"][\"USER\"],\n",
    "                                  password = settings.DATABASES[\"default\"][\"PASSWORD\"],\n",
    "                                  host = settings.DATABASES[\"default\"][\"HOST\"],\n",
    "                                  port = settings.DATABASES[\"default\"][\"PORT\"],\n",
    "                                  database = settings.DATABASES[\"default\"][\"NAME\"])\n",
    "connection.autocommit=True\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_where_items(exp_id, cols, table):\n",
    "    query = \"SELECT %s from %s where experiment_id = %s\" % (\", \".join(cols), table, exp_id) \n",
    "    cursor.execute(query)\n",
    "    where_items = cursor.fetchall()\n",
    "    return where_items\n",
    "\n",
    "def get_original_q_matrix():\n",
    "    # Get voted concepts per solution\n",
    "    concepts = SolutionConcept.objects.all()\n",
    "    MIN_THRESHOLD = 0.5\n",
    "    agreed_concepts = defaultdict(list)\n",
    "    agreed_concepts_len = {}\n",
    "    \n",
    "    # Count concept agreement per solution\n",
    "    count_per_solution = dict(concepts.values_list('solution__id').annotate(count=Count('user', distinct=True)))\n",
    "\n",
    "    # Filter out the concepts that didn't have agreement (50% of evaluators voted for it)\n",
    "    for solution_id in docs_id:\n",
    "        max_votes = count_per_solution[solution_id]\n",
    "        concepts_per_solution = dict(concepts.filter(solution__id=solution_id).values_list('concept').annotate(\n",
    "            count=Count(\"concept\")))\n",
    "        for concept, value in concepts_per_solution.items():\n",
    "            if value >= (max_votes * MIN_THRESHOLD):\n",
    "                agreed_concepts[solution_id].append(concept)\n",
    "        agreed_concepts_len[solution_id] = len(agreed_concepts[solution_id])\n",
    "        agreed_concepts_all = list(chain.from_iterable(agreed_concepts.values()))\n",
    "        agreed_concepts_set = set(agreed_concepts_all)\n",
    "        \n",
    "    concept_idx = np.asarray(list(agreed_concepts_set))\n",
    "    q_matrix = np.zeros((len(docs_id), len(concept_idx)))\n",
    "\n",
    "    for q_idx, question_id in enumerate(docs_id):\n",
    "        used_concepts = agreed_concepts[question_id]\n",
    "        q_matrix[q_idx, np.where(np.isin(concept_idx, used_concepts))] = 1\n",
    "    return q_matrix\n",
    "\n",
    "def transform_data(q_matrix, q_matrix_hat):\n",
    "    data = {}\n",
    "    # Calculate similarities among questions in Q-Matrix and estimated Q-Matrix\n",
    "    data['question_similarity'] = cosine_similarity(q_matrix)\n",
    "    data['question_distance'] = cosine_distances(q_matrix)\n",
    "    data['question_hat_similarity'] = cosine_similarity(q_matrix_hat)\n",
    "    data['question_hat_distance'] = cosine_distances(q_matrix_hat)\n",
    "    error = data['question_similarity'] - data['question_hat_similarity']\n",
    "\n",
    "    # Calculate total error, RMSE and CMD\n",
    "    data['error'] = np.sqrt(np.sum(np.power(error,2)))\n",
    "    data['rmse'] = np.sqrt(np.mean(np.power(error,2)))\n",
    "    qs = data['question_similarity']\n",
    "    qs_hat = data['question_hat_similarity']\n",
    "    data['cmd'] = 1-np.trace(np.dot(qs,qs_hat))/(np.linalg.norm(qs)*np.linalg.norm(qs_hat))\n",
    "    return data\n",
    "\n",
    "def calculate_anosim(item):\n",
    "    row = {}\n",
    "#     row['Experiment ID'] = item['exp_id']\n",
    "#     row['Method'] = \"%s (%d attempts)\" % (item['method'].upper(), item['attempts'])\n",
    "#     row['Concept'] = item['concepts']\n",
    "    dm = DistanceMatrix(item['question_distance'])\n",
    "    stats_list = []\n",
    "    p_values = []\n",
    "    for i in range(2, 20):\n",
    "        model = AgglomerativeClustering(n_clusters=i, \n",
    "#                                         affinity='cosine',\n",
    "                                        affinity='precomputed',\n",
    "                                        linkage='complete').fit(item['question_hat_distance'])\n",
    "#             item['q_matrix_hat'].T)\n",
    "        stats = anosim(dm, model.labels_, permutations=9999)\n",
    "        stats_list.append(stats['test statistic'])\n",
    "        p_values.append(stats['p-value'])\n",
    "    stats_list = np.asarray(stats_list)\n",
    "    p_values = np.asarray(p_values)\n",
    "    if np.any(np.where(p_values < 0.1)):\n",
    "        row['Statistical significant (p < 0.1)'] = True\n",
    "        row['Agg Concepts'] = np.where(p_values < 0.1)[0]+2\n",
    "#         row['R Statistic'] = np.asarray(stats_list)[np.where(np.asarray(p_values) < 0.1)]\n",
    "        sig_stats = stats_list[np.where(p_values < 0.1)]\n",
    "        row['R Statistic'] = \"%.2f at %d\" % (np.max(sig_stats), np.where(stats_list == np.max(sig_stats))[0]+2)\n",
    "    else:\n",
    "        row['Statistical significant (p < 0.1)'] = False\n",
    "        row['Agg Concepts'] = '--'\n",
    "        row['R Statistic'] = '--'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve original Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 5.72 ms, total: 161 ms\n",
      "Wall time: 6.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "q_matrix_original = get_original_q_matrix()\n",
    "q_matrix_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve automated Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 ms, sys: 4.54 ms, total: 70.6 ms\n",
      "Wall time: 456 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "exp_id = 26\n",
    "cols = [\"vectorizer\", \"min_df\", \"is_binary\", \"model\"]\n",
    "table = \"experiments_solution\"\n",
    "where_items = get_where_items(exp_id, cols, table)[0]\n",
    "v = eval(where_items[0])\n",
    "m = where_items[1]\n",
    "b = where_items[2]\n",
    "vectorizer_params={'ngram_range': (1,3)}\n",
    "train_data_features, vectorizer, _ = create_bag_of_words(solutions, v, binary=b, min_df=m, \n",
    "                                                         vectorizer_params=vectorizer_params)\n",
    "\n",
    "model = where_items[3]\n",
    "model_db = pickle.loads(base64.b64decode(model))\n",
    "q_matrix_automated = model_db.transform(train_data_features)\n",
    "# word_topic = model_db.components_.T\n",
    "data.append(transform_data(q_matrix_original, q_matrix_automated))\n",
    "q_matrix_automated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save automated Q-matrix\n",
    "with open('data/tese/q_matrix_automated.pkl', 'wb') as pklfile:\n",
    "    pickle.dump(q_matrix_automated, pklfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve refined Q-Matrix (automated after analysis)\n",
    "- 1 concept per solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_set = list(set(clusters))\n",
    "q_matrix_analysis = np.zeros((len(docs_id), len(set(clusters))))\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    q_matrix_analysis[idx, cluster_set.index(cluster)]= 1\n",
    "\n",
    "data.append(transform_data(q_matrix_original, q_matrix_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analyzed Q-matrix\n",
    "with open('data/tese/q_matrix_refined.pkl', 'wb') as pklfile:\n",
    "    pickle.dump(q_matrix_analysis, pklfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 s, sys: 0 ns, total: 31.5 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = []\n",
    "for item in data:\n",
    "    row = calculate_anosim(item)\n",
    "    df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistical significant (p &lt; 0.1)</th>\n",
       "      <th>Agg Concepts</th>\n",
       "      <th>R Statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...</td>\n",
       "      <td>0.20 at 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>0.18 at 19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Statistical significant (p < 0.1)  \\\n",
       "0                               True   \n",
       "1                               True   \n",
       "\n",
       "                                        Agg Concepts R Statistic  \n",
       "0  [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...  0.20 at 18  \n",
       "1  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  0.18 at 19  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = data[1]\n",
    "dm = DistanceMatrix(item['question_distance'])\n",
    "row = {}\n",
    "stats_list = []\n",
    "p_values = []\n",
    "for i in range(2, 20):\n",
    "    model = AgglomerativeClustering(n_clusters=i, \n",
    "                                    affinity='precomputed',\n",
    "                                    linkage='complete').fit(item['question_hat_distance'].T)\n",
    "    stats = anosim(dm, model.labels_, permutations=9999)\n",
    "    stats_list.append(stats['test statistic'])\n",
    "    p_values.append(stats['p-value'])\n",
    "stats_list = np.asarray(stats_list)\n",
    "p_values = np.asarray(p_values)\n",
    "if np.any(np.where(p_values < 0.1)):\n",
    "    row['Statistical significant (p < 0.1)'] = True\n",
    "    row['Agg Concepts'] = np.where(p_values < 0.1)[0]+2\n",
    "#         row['R Statistic'] = np.asarray(stats_list)[np.where(np.asarray(p_values) < 0.1)]\n",
    "    sig_stats = stats_list[np.where(p_values < 0.1)]\n",
    "    row['R Statistic'] = \"%.2f at %d\" % (np.max(sig_stats), np.where(stats_list == np.max(sig_stats))[0]+2)\n",
    "else:\n",
    "    row['Statistical significant (p < 0.1)'] = False\n",
    "    row['Agg Concepts'] = '--'\n",
    "    row['R Statistic'] = '--'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11479647, 0.09937463, 0.14540132, 0.11917605, 0.11783865,\n",
       "       0.11710581, 0.12025834, 0.12719094, 0.1247419 , 0.1293008 ,\n",
       "       0.14222523, 0.1538761 , 0.14789013, 0.14788238, 0.15228207,\n",
       "       0.1445512 , 0.1430687 , 0.17900104])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0163, 0.0304, 0.0014, 0.0047, 0.0052, 0.007 , 0.0073, 0.0051,\n",
       "       0.0088, 0.0074, 0.0051, 0.0056, 0.0062, 0.0075, 0.0078, 0.0106,\n",
       "       0.0115, 0.0037])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11479647, 0.09937463, 0.14540132, 0.11917605, 0.11783865,\n",
       "       0.11710581, 0.12025834, 0.12719094, 0.1247419 , 0.1293008 ,\n",
       "       0.14222523, 0.1538761 , 0.14789013, 0.14788238, 0.15228207,\n",
       "       0.1445512 , 0.1430687 , 0.17900104])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_list[np.where(p_values  < 0.1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
