{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB\n",
    "from questions.models import Solution, Cluster\n",
    "import psycopg2\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "#from tqdm import tqdm_notebook\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from vectorizer import NCutVectorizer\n",
    "from tokenizer import create_bag_of_words\n",
    "\n",
    "# Learning\n",
    "from clustering import Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems to be used: 132\n",
      "Solutions to be used: 54\n",
      "Got 54 documents\n"
     ]
    }
   ],
   "source": [
    "## Cleaning database\n",
    "last_id = 132\n",
    "# problems = Problem.objects.filter(id__gt=last_id)\n",
    "# # solutions_obj = Solution.objects.filter(problem__in=problems).update(ignore=True)\n",
    "# print(\"Problems to be ignored: %d\" % problems.count())\n",
    "\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "# problems = Problem.objects.all()\n",
    "print(\"Problems to be used: %d\" % problems.count())\n",
    "\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems, ignore=False).order_by('id')\n",
    "# solutions_obj = Solution.objects.all().order_by('id')\n",
    "print(\"Solutions to be used: %d\" % solutions_obj.count())\n",
    "\n",
    "docs_id = []\n",
    "questions = []\n",
    "solutions = []\n",
    "\n",
    "# Fill separated structures\n",
    "for sol in solutions_obj:\n",
    "    docs_id.append(sol.id)\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clusters(*args):\n",
    "    # Import libraries\n",
    "    # Helpers\n",
    "    import base64\n",
    "    import pickle\n",
    "    import time\n",
    "    import numpy as np\n",
    "    # Learning\n",
    "    from clustering import Clustering\n",
    "    # Evaluation\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "#     from gap import Gap\n",
    "    from coherence import calculate_umass_coherence\n",
    "    round_results = []\n",
    "    \n",
    "    \n",
    "    # Get arguments\n",
    "    dataset, v, m, b, ng, train_data_features, k, dist, method = args\n",
    "\n",
    "    # Instanciate objects\n",
    "    clustering = Clustering(train_data_features, k, metric=dist)\n",
    "#     gap = Gap(train_data_features, k, nrefs=20, distance=dist)\n",
    "\n",
    "    # Cluster\n",
    "    start = time.time()\n",
    "    model, document_topic, word_topic = getattr(clustering, method)()\n",
    "    clusters = document_topic.argmax(axis=1)\n",
    "    end = time.time()\n",
    "    clustering_time = end-start\n",
    "\n",
    "    # Compute Gap\n",
    "#     start = time.time()\n",
    "#     k_gap = gap.calculate_gap(clustering, method)\n",
    "#     end = time.time()\n",
    "#     gap_time = end-start\n",
    "\n",
    "#     # Compute silhouette. Keep single values to be able to plot it later\n",
    "#     start = time.time()\n",
    "#     try:\n",
    "#         k_silhouette = silhouette_score(train_data_features, clusters, metric=dist)\n",
    "#         # Compute the silhouette scores for each sample\n",
    "#         sample_silhouette_values = silhouette_samples(train_data_features, clusters, metric=dist)\n",
    "#     # Everything is assigned to one cluster\n",
    "#     except ValueError:\n",
    "#         k_silhouette = None\n",
    "#         sample_silhouette_values = []\n",
    "#     end = time.time()\n",
    "#     silhouette_time = end-start\n",
    "\n",
    "    # UMass coherence\n",
    "    start = time.time()\n",
    "    k_coherence5 = calculate_umass_coherence(train_data_features, word_topic, clusters, k, N=5)\n",
    "    k_coherence10 = calculate_umass_coherence(train_data_features, word_topic, clusters, k, N=10)\n",
    "    k_coherence15 = calculate_umass_coherence(train_data_features, word_topic, clusters, k, N=15)\n",
    "    end = time.time()\n",
    "    coherence_time = end-start\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": dataset,\n",
    "        \"X\": train_data_features,\n",
    "        \"y\": clusters,\n",
    "        \"vectorizer\": v.__name__,\n",
    "        \"is_binary\": b,\n",
    "        \"min_df\": m,\n",
    "        \"ngrams\": list(ng),\n",
    "        \"token\": train_data_features.shape[1],\n",
    "        \"distance\": dist,\n",
    "        \"k\": k,\n",
    "        \"method\": method,\n",
    "        \"model\": base64.b64encode(pickle.dumps(model)),\n",
    "        \"clustering_time\": clustering_time,\n",
    "#         \"gap\": k_gap[0],\n",
    "#         \"gap_std\": k_gap[1],\n",
    "#         \"gap_time\": gap_time,\n",
    "#         \"silhouette\": k_silhouette,\n",
    "#         \"silhouette_samples\": sample_silhouette_values,\n",
    "#         \"silhouette_time\": silhouette_time,\n",
    "        \"coherence_samples5\": k_coherence5[0],\n",
    "        \"coherence_samples10\": k_coherence10[0],\n",
    "        \"coherence_samples15\": k_coherence15[0],\n",
    "        \"coherence_med5\": k_coherence5[1],\n",
    "        \"coherence_med10\": k_coherence10[1],\n",
    "        \"coherence_med15\": k_coherence15[1],\n",
    "        \"coherence_std5\": k_coherence5[2],\n",
    "        \"coherence_std10\": k_coherence10[2],\n",
    "        \"coherence_std15\": k_coherence15[2],\n",
    "        \"coherence_time\": coherence_time,\n",
    "        \"coherence_k5\": len(k_coherence5[0]),\n",
    "        \"coherence_k10\": len(k_coherence10[0]),\n",
    "        \"coherence_k15\": len(k_coherence15[0]),\n",
    "    }\n",
    "\n",
    "        # Connect to DB\n",
    "    #    connection = psycopg2.connect(user = settings.DATABASES[\"default\"][\"USER\"],\n",
    "    #                                  password = settings.DATABASES[\"default\"][\"PASSWORD\"],\n",
    "    #                                  host = settings.DATABASES[\"default\"][\"HOST\"],\n",
    "    #                                  port = settings.DATABASES[\"default\"][\"PORT\"],\n",
    "    #                                  database = settings.DATABASES[\"default\"][\"NAME\"])\n",
    "    #    connection.autocommit=True\n",
    "    #    cursor = connection.cursor()\n",
    "\n",
    "    # Write PSQL query\n",
    "    insert_query_base = \"INSERT INTO EXPERIMENTS_2020_02_12 \"\n",
    "    column_value = []\n",
    "    insert_format = []\n",
    "    query_values = []\n",
    "    for col in row.keys():\n",
    "        if isinstance(row[col], np.ndarray):\n",
    "            query_values.append(row[col].tolist())\n",
    "        else:\n",
    "            query_values.append(row[col])\n",
    "        column_value.append(col)\n",
    "        insert_format.append(\"%s\")\n",
    "\n",
    "    insert_query = insert_query_base + \"(\" + \", \".join(column_value) + \") VALUES \"\n",
    "    insert_query += \"(\" + \", \".join(insert_format) + \")\"\n",
    "    query_values = tuple(query_values)\n",
    "    return insert_query, query_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 s, sys: 46.9 ms, total: 2.83 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers = [\n",
    "    #CountVectorizer,\n",
    "    #TfidfVectorizer, \n",
    "    NCutVectorizer\n",
    "]\n",
    "ngrams = [\n",
    "    #(1,1), # unigrams\n",
    "    #(1,2), # unigrams + bigrams\n",
    "    (1,3), # unigrams + bigrams + trigrams\n",
    "    #(2,2), # bigrams\n",
    "    #(2,3), # bigrams + trigrams\n",
    "    #(3,3), # trigrams\n",
    "]\n",
    "#min_df = np.arange(0.05, 0.51, 0.05)\n",
    "min_df = [0.35]\n",
    "binary = [\n",
    "    True,\n",
    "    #False\n",
    "]\n",
    "cluster_methods = [\n",
    "    'nmf',\n",
    "    #'lda'\n",
    "]\n",
    "#, 'hierarchical', 'gaussian_mixture', 'spectral_clustering']\n",
    "metric = [\n",
    "    'euclidean', \n",
    "#     'cosine', \n",
    "#     'correlation'\n",
    "]\n",
    "\n",
    "# Sequences to be sent to map function\n",
    "skipped = []\n",
    "args = []\n",
    "\n",
    "# Grid search\n",
    "# Add jobs to list\n",
    "for dataset in range(100):\n",
    "    #for v in tqdm_notebook(vectorizers, desc=\"vectorizer\", leave=False):\n",
    "    for v in vectorizers:\n",
    "        #for m in tqdm_notebook(min_df, desc=\"min_df\", leave=False):\n",
    "        for m in min_df:\n",
    "            #for b in tqdm_notebook(binary, desc=\"binary\", leave=False):\n",
    "            for b in binary:\n",
    "                #for ng in tqdm_notebook(ngrams, desc=\"ngrams\", leave=False):\n",
    "                for ng in ngrams:\n",
    "                    try:\n",
    "                        train_data_features,_,_ = create_bag_of_words(solutions, v, binary=b, min_df=m, \n",
    "                                                                      vectorizer_params={'ngram_range': ng})\n",
    "                    except ValueError:\n",
    "                        skipped.append((v,m,b,ng))\n",
    "                        continue\n",
    "\n",
    "                    # Remove rows containing only zeros (weird exercises)\n",
    "                    # As we're using less tokens, some rows may be excluded\n",
    "                    solution_sample = train_data_features[~(train_data_features==0).all(1)]\n",
    "#                    if solution_sample.shape != train_data_features.shape:\n",
    "#                        error = {\n",
    "#                            \"vectorizer\": v,\n",
    "#                            \"min_df\": m,\n",
    "#                            \"binary\": b\n",
    "#                        }\n",
    "#                        print(\"ERROR: %s\" % error)\n",
    "        #             clusters = range(2, int(np.sqrt(min(train_data_features.shape)))+1)\n",
    "                    #clusters = range(2, 16)\n",
    "                    clusters = [7]\n",
    "                    #for k in tqdm_notebook(clusters, desc=\"clusters\", leave=False):\n",
    "                    for k in clusters:\n",
    "#                        for dist in tqdm_notebook(metric, desc=\"metric\", leave=False):\n",
    "                        dist=\"euclidean\"\n",
    "                        #for method in tqdm_notebook(cluster_methods, desc=\"method\", leave=False):\n",
    "                        for method in cluster_methods:\n",
    "                            arg_map = ('solution_2020_02_12_%2d' % dataset,\n",
    "                                        v,\n",
    "                                        m,\n",
    "                                        b,\n",
    "                                        ng,\n",
    "                                        solution_sample,\n",
    "                                        k,\n",
    "                                        dist,\n",
    "                                        method\n",
    "                                      )\n",
    "                            args.append(arg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.52 s, sys: 15.6 ms, total: 3.53 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Insert to DB\n",
    "connection = psycopg2.connect(user = settings.DATABASES[\"default\"][\"USER\"],\n",
    "                          password = settings.DATABASES[\"default\"][\"PASSWORD\"],\n",
    "                          host = settings.DATABASES[\"default\"][\"HOST\"],\n",
    "                          port = settings.DATABASES[\"default\"][\"PORT\"],\n",
    "                          database = settings.DATABASES[\"default\"][\"NAME\"])\n",
    "connection.autocommit=True\n",
    "cursor = connection.cursor()\n",
    "\n",
    "for item in args[next_idx:]:\n",
    "    insert_query, query_values = run_clusters(*item)\n",
    "    query = cursor.mogrify(insert_query, query_values)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
