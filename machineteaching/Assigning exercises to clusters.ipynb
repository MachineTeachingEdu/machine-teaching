{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 26\n",
    "\n",
    "- Min DF: 0.05\n",
    "- Binary: True\n",
    "- Vectorizer: Count\n",
    "- Method: LDA\n",
    "- Best k: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "# from db import PythonProblems\n",
    "import io\n",
    "\n",
    "# DB\n",
    "from questions.models import Solution, Cluster\n",
    "import psycopg2\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "import pickle\n",
    "import base64\n",
    "\n",
    "# Preprocessing\n",
    "from tokenizer import create_bag_of_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Learning\n",
    "from clustering import Clustering\n",
    "from analyzer import python_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems to be ignored: 591\n",
      "Problems to be used: 132\n",
      "Solutions to be used: 54\n",
      "Got 54 documents\n"
     ]
    }
   ],
   "source": [
    "## Cleaning database\n",
    "last_id = 132\n",
    "problems = Problem.objects.filter(id__gt=last_id)\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems).update(ignore=True)\n",
    "print(\"Problems to be ignored: %d\" % problems.count())\n",
    "\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "# problems = Problem.objects.all()\n",
    "print(\"Problems to be used: %d\" % problems.count())\n",
    "\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems, ignore=False).order_by('id')\n",
    "# solutions_obj = Solution.objects.all().order_by('id')\n",
    "print(\"Solutions to be used: %d\" % solutions_obj.count())\n",
    "\n",
    "docs_id = []\n",
    "questions = []\n",
    "solutions = []\n",
    "\n",
    "# Fill separated structures\n",
    "for sol in solutions_obj:\n",
    "    docs_id.append(sol.id)\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(user = \"machineteaching\",\n",
    "                                  password = \"***REMOVED***\",\n",
    "                                  host = \"localhost\",\n",
    "#                                   port = \"5432\",\n",
    "                                  database = \"machineteaching\")\n",
    "connection.autocommit=True\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_where_items(exp_id):\n",
    "    cols = [\"vectorizer\", \"min_df\", \"is_binary\", \"distance\", \"method\", \"dataset\", \"k\", \"model\", \"X\"]\n",
    "    query = \"SELECT %s from experiments_solution where experiment_id = %s\" % (\", \".join(cols), exp_id) \n",
    "    cursor.execute(query)\n",
    "    where_items = cursor.fetchall()\n",
    "    return where_items\n",
    "\n",
    "def analyze(solutions, where_items, exp_id):\n",
    "    v = eval(where_items[0][0])\n",
    "    m = where_items[0][1]\n",
    "    b = where_items[0][2]\n",
    "    dist = where_items[0][3]\n",
    "    method = where_items[0][4]\n",
    "    k = where_items[0][6]\n",
    "    model_db = pickle.loads(base64.b64decode(where_items[0][7]))\n",
    "    X = np.asarray(where_items[0][8])\n",
    "\n",
    "    train_data_features, vectorizer, feature_names = create_bag_of_words(solutions, v, binary=b, min_df=m)\n",
    "    clustering = Clustering(train_data_features, k, metric=dist)\n",
    "    clustering.seed = model_db.random_state\n",
    "    \n",
    "    model, document_topic, word_topic = getattr(clustering, method)()\n",
    "    \n",
    "    return document_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions\n",
      "('CountVectorizer', 0.05, True, 'euclidean', 'lda', 'solution_all', 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmoraes/miniconda3/envs/machineteaching/lib/python3.7/site-packages/sklearn/base.py:253: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.20.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get experiment conditions\n",
    "exp_id = 26\n",
    "where_items = get_where_items(exp_id)\n",
    "print(\"Conditions\")\n",
    "print(where_items[0][0:7])\n",
    "\n",
    "document_topic = analyze(solutions, where_items, exp_id)\n",
    "document_clusters = document_topic.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_def = {\n",
    "    4: \"String manipulation\",\n",
    "    6: \"One-liners\",\n",
    "    8: \"Conditional\",\n",
    "    10: \"Array manipulation\",\n",
    "    12: \"Loops and nested loops\"\n",
    "}\n",
    "\n",
    "for key,value in clusters_def.items():\n",
    "    cluster = Cluster(id=key, label=value)\n",
    "    cluster.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign solutions to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all clusters\n",
    "for item in Solution.objects.filter(cluster__isnull=False):\n",
    "    item.cluster=None\n",
    "    item.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution 770 from cluster 1 was not assigned\n",
      "Solution 772 from cluster 7 was not assigned\n",
      "Solution 786 from cluster 1 was not assigned\n",
      "Solution 806 from cluster 5 was not assigned\n",
      "Solution 808 from cluster 3 was not assigned\n"
     ]
    }
   ],
   "source": [
    "clusters_merge = {\n",
    "    2: 4\n",
    "}\n",
    "\n",
    "for idx, doc_id in enumerate(docs_id):\n",
    "    # Assigning docs to valid clusters\n",
    "    if (document_clusters[idx]+1) in clusters_def.keys():\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        cluster = Cluster.objects.get(pk=(document_clusters[idx]+1))\n",
    "        solution.cluster=cluster\n",
    "        solution.save()\n",
    "    elif (document_clusters[idx]+1) in clusters_merge.keys():\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        cluster = Cluster.objects.get(pk=(clusters_merge[document_clusters[idx]+1]))\n",
    "        solution.cluster=cluster\n",
    "        solution.save()\n",
    "    else:\n",
    "        print(\"Solution %d from cluster %d was not assigned\" % (doc_id, document_clusters[idx]+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
