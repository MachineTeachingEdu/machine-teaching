{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 26\n",
    "\n",
    "- Min DF: 0.05\n",
    "- Binary: True\n",
    "- Vectorizer: Count\n",
    "- Method: LDA\n",
    "- Best k: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "# from db import PythonProblems\n",
    "import io\n",
    "\n",
    "# DB\n",
    "from questions.models import Solution, Cluster\n",
    "import psycopg2\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "import pickle\n",
    "import base64\n",
    "\n",
    "# Preprocessing\n",
    "from tokenizer import create_bag_of_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Learning\n",
    "from clustering import Clustering\n",
    "from analyzer import python_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems to be ignored: 597\n",
      "Problems to be used: 132\n",
      "Solutions to be used: 54\n",
      "Got 54 documents\n"
     ]
    }
   ],
   "source": [
    "## Cleaning database\n",
    "last_id = 132\n",
    "problems = Problem.objects.filter(id__gt=last_id)\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems).update(ignore=True)\n",
    "print(\"Problems to be ignored: %d\" % problems.count())\n",
    "\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "# problems = Problem.objects.all()\n",
    "print(\"Problems to be used: %d\" % problems.count())\n",
    "\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems, ignore=False).order_by('id')\n",
    "# solutions_obj = Solution.objects.all().order_by('id')\n",
    "print(\"Solutions to be used: %d\" % solutions_obj.count())\n",
    "\n",
    "docs_id = []\n",
    "questions = []\n",
    "solutions = []\n",
    "\n",
    "# Fill separated structures\n",
    "for sol in solutions_obj:\n",
    "    docs_id.append(sol.id)\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(user = \"machineteaching\",\n",
    "                                  password = \"***REMOVED***\",\n",
    "                                  host = \"localhost\",\n",
    "#                                   port = \"5432\",\n",
    "                                  database = \"machineteaching\")\n",
    "connection.autocommit=True\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_where_items(exp_id):\n",
    "    cols = [\"vectorizer\", \"min_df\", \"is_binary\", \"distance\", \"method\", \"dataset\", \"k\", \"model\", \"X\"]\n",
    "    query = \"SELECT %s from experiments_solution where experiment_id = %s\" % (\", \".join(cols), exp_id) \n",
    "    cursor.execute(query)\n",
    "    where_items = cursor.fetchall()\n",
    "    return where_items\n",
    "\n",
    "def analyze(solutions, where_items, exp_id):\n",
    "    v = eval(where_items[0][0])\n",
    "    m = where_items[0][1]\n",
    "    b = where_items[0][2]\n",
    "    dist = where_items[0][3]\n",
    "    method = where_items[0][4]\n",
    "    k = where_items[0][6]\n",
    "    model_db = pickle.loads(base64.b64decode(where_items[0][7]))\n",
    "    X = np.asarray(where_items[0][8])\n",
    "\n",
    "    train_data_features, vectorizer, feature_names = create_bag_of_words(solutions, v, binary=b, min_df=m)\n",
    "    clustering = Clustering(train_data_features, k, metric=dist)\n",
    "    clustering.seed = model_db.random_state\n",
    "    \n",
    "    model, document_topic, word_topic = getattr(clustering, method)()\n",
    "    \n",
    "    return document_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions\n",
      "('CountVectorizer', 0.05, True, 'euclidean', 'lda', 'solution_all', 12)\n"
     ]
    }
   ],
   "source": [
    "# Get experiment conditions\n",
    "exp_id = 26\n",
    "where_items = get_where_items(exp_id)\n",
    "print(\"Conditions\")\n",
    "print(where_items[0][0:7])\n",
    "\n",
    "document_topic = analyze(solutions, where_items, exp_id)\n",
    "document_clusters = document_topic.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_def = {\n",
    "    4: \"String manipulation\",\n",
    "    6: \"Math functions\",\n",
    "    8: \"Conditional structure\",\n",
    "    10: \"List loops\",\n",
    "    12: \"Math and string loops\"\n",
    "}\n",
    "\n",
    "for key,value in clusters_def.items():\n",
    "    cluster = Cluster(id=key, label=value)\n",
    "    cluster.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign solutions to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear all clusters\n",
    "for item in Solution.objects.filter(cluster__isnull=False):\n",
    "    item.cluster=None\n",
    "    item.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution 770 from cluster 1 was not assigned\n",
      "Assigning to 2nd best: 6\n",
      "Solution 772 from cluster 7 was not assigned\n",
      "Assigning to 2nd best: 8\n",
      "Solution 786 from cluster 1 was not assigned\n",
      "Assigning to 2nd best: 6\n",
      "Solution 806 from cluster 5 was not assigned\n",
      "Assigning to 2nd best: 6\n",
      "Solution 808 from cluster 3 was not assigned\n",
      "Assigning to 2nd best: 6\n"
     ]
    }
   ],
   "source": [
    "clusters_merge = {\n",
    "    2: 4\n",
    "}\n",
    "\n",
    "for idx, doc_id in enumerate(docs_id):\n",
    "    # Assigning docs to valid clusters\n",
    "    if (document_clusters[idx]+1) in clusters_def.keys():\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        cluster = Cluster.objects.get(pk=(document_clusters[idx]+1))\n",
    "        solution.cluster=cluster\n",
    "#         solution.save()\n",
    "    elif (document_clusters[idx]+1) in clusters_merge.keys():\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        cluster = Cluster.objects.get(pk=(clusters_merge[document_clusters[idx]+1]))\n",
    "        solution.cluster=cluster\n",
    "#         solution.save()\n",
    "    # Assign 2nd best value\n",
    "    else:\n",
    "        print(\"Solution %d from cluster %d was not assigned\" % (doc_id, document_clusters[idx]+1))\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        max_idx = np.argsort(document_topic[idx])[::-1]\n",
    "        cluster = Cluster.objects.get(pk=(max_idx[1]+1))\n",
    "        print(\"Assigning to 2nd best: %d\" % cluster.pk)\n",
    "        solution.cluster=cluster\n",
    "        solution.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning new solutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00308647 0.00308648 0.00308647 0.00308646 0.00308645 0.00308659\n",
      "  0.00308646 0.96604881 0.00308642 0.00308647 0.00308642 0.0030865 ]]\n",
      "8\n",
      "6\n",
      "12\n",
      "2\n",
      "10\n",
      "1\n",
      "3\n",
      "4\n",
      "7\n",
      "5\n",
      "11\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def assign_to_cluster(exp_id, solutions, exercise_sol):\n",
    "    where_items = get_where_items(exp_id)\n",
    "    v = eval(where_items[0][0])\n",
    "    m = where_items[0][1]\n",
    "    b = where_items[0][2]\n",
    "    model_db = pickle.loads(base64.b64decode(where_items[0][7]))\n",
    "    _, vectorizer, _ = create_bag_of_words(solutions, v, binary=b, min_df=m)\n",
    "    train_data_features = vectorizer.transform(exercise_sol)\n",
    "    document_topic = model_db.transform(train_data_features)\n",
    "    return document_topic\n",
    "\n",
    "exercise_sol = [\"\"\"\n",
    "def avioes(c, p_compr, p_compet):\n",
    "    if p_compr//p_compet >= c:\n",
    "        return 'Suficiente'\n",
    "    else:\n",
    "        return 'Insuficiente'\n",
    "\"\"\"]\n",
    "\n",
    "\n",
    "\n",
    "document_topic = assign_to_cluster(exp_id, solutions, exercise_sol)\n",
    "print(document_topic)\n",
    "# print(document_clusters)\n",
    "max_idx = np.argsort(document_topic[0])[::-1]\n",
    "for i in max_idx:\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def generate():\n",
    "    num_tests = 20\n",
    "    tests = []\n",
    "    for test in range(num_tests):\n",
    "        idade = random.randrange(10, 80)\n",
    "        estudante = bool(random.getrandbits(1))\n",
    "        test_case = [idade, estudante]\n",
    "        tests.append(test_case)\n",
    "    return json.dumps(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[48, false], [53, false], [43, true], [64, true], [57, true], [15, false], [47, true], [76, false], [18, false], [35, true], [42, true], [29, false], [56, true], [70, false], [20, true], [66, true], [76, false], [56, false], [32, false], [29, true]]'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4]\n",
      "[2, 1]\n",
      "[9, 0]\n",
      "[1, 0]\n",
      "[3, 5]\n",
      "[5, 0]\n",
      "[3, 6]\n",
      "[9, 2]\n",
      "[4, 0]\n",
      "[4, 8]\n",
      "[7, 9]\n",
      "[9, 2]\n",
      "[8, 5]\n",
      "[3, 8]\n",
      "[5, 8]\n",
      "[5, 7]\n",
      "[9, 0]\n",
      "[7, 7]\n",
      "[2, 5]\n",
      "[2, 5]\n"
     ]
    }
   ],
   "source": [
    "cases = generate()\n",
    "for i in cases:\n",
    "    i = i.replace('\\n','')\n",
    "    i = i.split(' ')\n",
    "    new_i = []\n",
    "    for elem in i:\n",
    "        new_i.append(int(elem))\n",
    "    print(new_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
