{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 26\n",
    "\n",
    "- Min DF: 0.05\n",
    "- Binary: True\n",
    "- Vectorizer: Count\n",
    "- Method: LDA\n",
    "- Best k: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "# from db import PythonProblems\n",
    "import io\n",
    "\n",
    "# DB\n",
    "from questions.models import Solution, Cluster\n",
    "import psycopg2\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "import pickle\n",
    "import base64\n",
    "\n",
    "# Preprocessing\n",
    "from tokenizer import create_bag_of_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Learning\n",
    "from clustering import Clustering\n",
    "from analyzer import python_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems to be ignored: 610\n",
      "Problems to be used: 132\n",
      "Solutions to be used: 54\n",
      "Got 54 documents\n"
     ]
    }
   ],
   "source": [
    "## Cleaning database\n",
    "last_id = 132\n",
    "problems = Problem.objects.filter(id__gt=last_id)\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems).update(ignore=True)\n",
    "print(\"Problems to be ignored: %d\" % problems.count())\n",
    "\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "# problems = Problem.objects.all()\n",
    "print(\"Problems to be used: %d\" % problems.count())\n",
    "\n",
    "solutions_obj = Solution.objects.filter(problem__in=problems, ignore=False).order_by('id')\n",
    "# solutions_obj = Solution.objects.all().order_by('id')\n",
    "print(\"Solutions to be used: %d\" % solutions_obj.count())\n",
    "\n",
    "docs_id = []\n",
    "questions = []\n",
    "solutions = []\n",
    "\n",
    "# Fill separated structures\n",
    "for sol in solutions_obj:\n",
    "    docs_id.append(sol.id)\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(user = \"machineteaching\",\n",
    "                                  password = \"\",\n",
    "                                  host = \"localhost\",\n",
    "#                                   port = \"5432\",\n",
    "                                  database = \"machineteaching\")\n",
    "connection.autocommit=True\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_where_items(exp_id):\n",
    "    cols = [\"vectorizer\", \"min_df\", \"is_binary\", \"distance\", \"method\", \"dataset\", \"k\", \"model\", \"X\"]\n",
    "    query = \"SELECT %s from experiments_solution where experiment_id = %s\" % (\", \".join(cols), exp_id) \n",
    "    cursor.execute(query)\n",
    "    where_items = cursor.fetchall()\n",
    "    return where_items\n",
    "\n",
    "def analyze(solutions, where_items, exp_id):\n",
    "    v = eval(where_items[0][0])\n",
    "    m = where_items[0][1]\n",
    "    b = where_items[0][2]\n",
    "    dist = where_items[0][3]\n",
    "    method = where_items[0][4]\n",
    "    k = where_items[0][6]\n",
    "    model_db = pickle.loads(base64.b64decode(where_items[0][7]))\n",
    "    X = np.asarray(where_items[0][8])\n",
    "\n",
    "    train_data_features, vectorizer, feature_names = create_bag_of_words(solutions, v, binary=b, min_df=m)\n",
    "    clustering = Clustering(train_data_features, k, metric=dist)\n",
    "    clustering.seed = model_db.random_state\n",
    "    \n",
    "    model, document_topic, word_topic = getattr(clustering, method)()\n",
    "    \n",
    "    return document_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditions\n",
      "('CountVectorizer', 0.05, True, 'euclidean', 'lda', 'solution_all', 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmoraes/miniconda3/envs/machineteaching/lib/python3.7/site-packages/sklearn/base.py:253: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.20.1 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get experiment conditions\n",
    "exp_id = 26\n",
    "where_items = get_where_items(exp_id)\n",
    "print(\"Conditions\")\n",
    "print(where_items[0][0:7])\n",
    "\n",
    "document_topic = analyze(solutions, where_items, exp_id)\n",
    "document_clusters = document_topic.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_def = {\n",
    "    4: \"String manipulation\",\n",
    "    6: \"Math functions\",\n",
    "    8: \"Conditional structure\",\n",
    "    10: \"List loops\",\n",
    "    12: \"Math and string loops\"\n",
    "}\n",
    "\n",
    "for key,value in clusters_def.items():\n",
    "    cluster = Cluster(id=key, label=value)\n",
    "    cluster.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign solutions to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear all clusters\n",
    "for item in Solution.objects.filter(cluster__isnull=False):\n",
    "    item.cluster=None\n",
    "    item.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution 770 from cluster 1 was not assigned\n",
      "Assigning to 2nd best: 6\n",
      "Solution 772 from cluster 7 was not assigned\n",
      "Assigning to 2nd best: 8\n",
      "Solution 786 from cluster 1 was not assigned\n",
      "Assigning to 2nd best: 6\n",
      "Solution 806 from cluster 5 was not assigned\n",
      "Assigning to 2nd best: 6\n",
      "Solution 808 from cluster 3 was not assigned\n",
      "Assigning to 2nd best: 6\n"
     ]
    }
   ],
   "source": [
    "clusters_merge = {\n",
    "    2: 4\n",
    "}\n",
    "\n",
    "for idx, doc_id in enumerate(docs_id):\n",
    "    # Assigning docs to valid clusters\n",
    "    if (document_clusters[idx]+1) in clusters_def.keys():\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        cluster = Cluster.objects.get(pk=(document_clusters[idx]+1))\n",
    "        solution.cluster=cluster\n",
    "#         solution.save()\n",
    "    elif (document_clusters[idx]+1) in clusters_merge.keys():\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        cluster = Cluster.objects.get(pk=(clusters_merge[document_clusters[idx]+1]))\n",
    "        solution.cluster=cluster\n",
    "#         solution.save()\n",
    "    # Assign 2nd best value\n",
    "    else:\n",
    "        print(\"Solution %d from cluster %d was not assigned\" % (doc_id, document_clusters[idx]+1))\n",
    "        solution = Solution.objects.get(pk=doc_id)\n",
    "        max_idx = np.argsort(document_topic[idx])[::-1]\n",
    "        cluster = Cluster.objects.get(pk=(max_idx[1]+1))\n",
    "        print(\"Assigning to 2nd best: %d\" % cluster.pk)\n",
    "        solution.cluster=cluster\n",
    "        solution.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning new solutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09901917 0.0064103  0.00641039 0.0064103  0.00641047 0.83687736\n",
      "  0.00641032 0.00641038 0.00641026 0.00641034 0.00641026 0.00641046]]\n",
      "6\n",
      "1\n",
      "5\n",
      "12\n",
      "3\n",
      "8\n",
      "10\n",
      "7\n",
      "4\n",
      "2\n",
      "11\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def assign_to_cluster(exp_id, solutions, exercise_sol):\n",
    "    where_items = get_where_items(exp_id)\n",
    "    v = eval(where_items[0][0])\n",
    "    m = where_items[0][1]\n",
    "    b = where_items[0][2]\n",
    "    model_db = pickle.loads(base64.b64decode(where_items[0][7]))\n",
    "    _, vectorizer, _ = create_bag_of_words(solutions, v, binary=b, min_df=m)\n",
    "    train_data_features = vectorizer.transform(exercise_sol)\n",
    "    document_topic = model_db.transform(train_data_features)\n",
    "    return document_topic\n",
    "\n",
    "exercise_sol = [\"\"\"\n",
    "def intercala(l1, l2):\n",
    "    return [l1[0], l2[0], l1[1], l2[1], l1[2], l2[2]]\n",
    "\"\"\"]\n",
    "\n",
    "\n",
    "\n",
    "document_topic = assign_to_cluster(exp_id, solutions, exercise_sol)\n",
    "print(document_topic)\n",
    "# print(document_clusters)\n",
    "max_idx = np.argsort(document_topic[0])[::-1]\n",
    "for i in max_idx:\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    import random\n",
    "    num_tests = 10\n",
    "    tests = []\n",
    "    for test in range(num_tests):\n",
    "        l1 = random.sample(range(10), 3)\n",
    "        l2 = random.sample(range(10), 3)\n",
    "        test_case = [l1, l2]\n",
    "        tests.append(test_case)\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[7, 0, 3], [7, 4, 0]],\n",
       " [[3, 4, 0], [7, 6, 3]],\n",
       " [[9, 7, 8], [7, 2, 6]],\n",
       " [[1, 6, 9], [4, 3, 9]],\n",
       " [[5, 8, 9], [0, 9, 5]],\n",
       " [[1, 3, 9], [6, 2, 9]],\n",
       " [[7, 0, 3], [6, 0, 2]],\n",
       " [[5, 3, 9], [5, 8, 2]],\n",
       " [[1, 5, 3], [7, 4, 1]],\n",
       " [[8, 1, 2], [9, 3, 6]]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases = generate()\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('humildade', 10, 1.7016920265272026)]\n",
      "(['humildade'], [10, 1.7016920265272026])\n",
      "[(8.607756810640138, 6, (4+5j))]\n",
      "([], [8.607756810640138, 6, (4+5j)])\n",
      "[('entretendo', (6+4j), 7)]\n",
      "(['entretendo'], [(6+4j), 7])\n"
     ]
    }
   ],
   "source": [
    "def solve(tupla):\n",
    "    strings = []\n",
    "    numeros = []\n",
    "    \n",
    "    if type(tupla[0]) == str:\n",
    "        strings.append(tupla[0])\n",
    "    else:\n",
    "        numeros.append(tupla[0])\n",
    "        \n",
    "    if type(tupla[1]) == str:\n",
    "        strings.append(tupla[1])\n",
    "    else:\n",
    "        numeros.append(tupla[1])\n",
    "        \n",
    "    if type(tupla[2]) == str:\n",
    "        strings.append(tupla[2])\n",
    "    else:\n",
    "        numeros.append(tupla[2])\n",
    "        \n",
    "    return strings, numeros\n",
    "for dataset in cases:\n",
    "    print(dataset)\n",
    "    print(solve(*dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
