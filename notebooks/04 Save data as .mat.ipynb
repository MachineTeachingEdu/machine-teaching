{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "from db import PythonProblems\n",
    "import io\n",
    "from scipy import io as sio\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Preprocessing\n",
    "import tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = PythonProblems('python.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 758 documents\n",
      "Success in parsing all documents! You may go on!\n"
     ]
    }
   ],
   "source": [
    "removed_itens = ['NEWLINE', 'STRING', 'ENDMARKER', 'NUMBER', 'INDENT', 'DEDENT', \"NL\", 'COMMENT', 'ERRORTOKEN']\n",
    "allowed_itens = ['NAME', 'OP']\n",
    "cursor = db.conn.cursor()\n",
    "docs = []\n",
    "docs_id = []\n",
    "docs_category = []\n",
    "errors = []\n",
    "\n",
    "# lendo os dados\n",
    "cursor.execute(\"\"\"\n",
    "SELECT solution.id, solution.content, problem.category FROM solution, problem where solution.problem_id = problem.id;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "for idx, row in enumerate(cursor.fetchall()):\n",
    "    file = io.StringIO(row[1])\n",
    "    doc = []\n",
    "    try:\n",
    "        for item in tokenize.generate_tokens(file.readline):\n",
    "            if tokenize.tok_name[item[0]] not in removed_itens:\n",
    "                if tokenize.tok_name[item[0]] in allowed_itens:\n",
    "                    doc.append(item[1])\n",
    "                else:\n",
    "                    print(\"%s %s\" % (tokenize.tok_name[item[0]], item[1]))\n",
    "    except (IndentationError, tokenize.TokenError):\n",
    "        errors.append(\"Please, fix solution %d before continuing\" % (idx+1))\n",
    "        \n",
    "    docs.append(' '.join(doc))\n",
    "    docs_id.append(row[0])\n",
    "    docs_category.append(row[2])\n",
    "\n",
    "print(\"Got %d documents\" %(idx+1))\n",
    "\n",
    "if not errors:\n",
    "    print(\"Success in parsing all documents! You may go on!\")\n",
    "else:\n",
    "    for item in errors:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing solutions into bag of words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(758, 27)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = ['print'],   \\\n",
    "                             #max_features = 26d,\n",
    "                             binary=False,\n",
    "                             min_df=0.05\n",
    "                            ) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "# Document-term matrix\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab_cell = np.asarray(vocab).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set with class string has 121 observations.\n",
      "17 obs were removed. Total: 104\n",
      "Train set with class math has 82 observations.\n",
      "2 obs were removed. Total: 80\n",
      "Train set with class loop has 13 observations.\n",
      "0 obs were removed. Total: 13\n",
      "Train set with class list has 48 observations.\n",
      "11 obs were removed. Total: 37\n",
      "Train set with class function has 29 observations.\n",
      "1 obs were removed. Total: 28\n",
      "Train set with class file has 4 observations.\n",
      "0 obs were removed. Total: 4\n",
      "Train set with class dict has 38 observations.\n",
      "6 obs were removed. Total: 32\n",
      "Train set with class conditional has 13 observations.\n",
      "0 obs were removed. Total: 13\n",
      "Train set with class None has 410 observations.\n",
      "33 obs were removed. Total: 377\n"
     ]
    }
   ],
   "source": [
    "categories = list(set(docs_category))\n",
    "none_idx = categories.index(None)\n",
    "categories[none_idx] = \"None\"\n",
    "categories.sort(reverse=True)\n",
    "data_array = np.empty((len(categories), 1), dtype=object)\n",
    "\n",
    "for i, item in enumerate(categories):\n",
    "    if item == \"None\":\n",
    "        item = None\n",
    "    idx = np.where(np.array(docs_category) == item)\n",
    "    print(\"Train set with class %s has %d observations.\" %(item, len(idx[0])))\n",
    "    train = train_data_features[idx]\n",
    "    to_remove = np.where(~train.any(axis=1))[0]\n",
    "    train = np.array([np.delete(train, to_remove, 0)]).astype(np.double)\n",
    "    print(\"%d obs were removed. Total: %d\" %(len(to_remove), len(train[0])))\n",
    "    data_array[i] = [train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('python.mat', mdict={'train_set': data_array.T, 'marc_label': vocab_cell, 'test_set': data_array.T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_none = np.delete(data_array, (8), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('python_none.mat', mdict={'train_set': data_array_none.T, 'marc_label': vocab_cell, 'test_set': data_array_none.T})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
