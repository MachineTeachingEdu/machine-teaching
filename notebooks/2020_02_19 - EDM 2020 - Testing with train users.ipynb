{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Helpers\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "#DB\n",
    "from django.db.models import Case, IntegerField, Value\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# DB \n",
    "import psycopg2\n",
    "from django.conf import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"data/edm2020/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get student data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter used problems and solutions\n",
    "last_id = 132\n",
    "problems = Problem.objects.filter(id__lte=last_id)\n",
    "\n",
    "# Get all students\n",
    "user_blacklist = UserProfile.objects.filter(professor__user__username='sem_professor')\n",
    "\n",
    "attempts = UserLog.objects.exclude(outcome='S').filter(\n",
    "    timestamp__lt=OuterRef('timestamp'), user__id=OuterRef('user__id')).annotate(\n",
    "    attempt=Count('*')).values('attempt')\n",
    "\n",
    "# Remove group by values\n",
    "attempts.query.set_group_by()\n",
    "\n",
    "# Get users\n",
    "users = UserLog.objects.filter(problem__in=problems).exclude(outcome='S').exclude(\n",
    "    user__userprofile__in=user_blacklist).annotate(\n",
    "    attempt=Subquery(attempts, output_field=IntegerField())).annotate(\n",
    "    score=Case(\n",
    "        When(outcome='F', then=Value(0.1)),\n",
    "        When(outcome='P', then=Value(1)),\n",
    "        output_field=IntegerField())).values_list(\n",
    "    \"user__id\", \"problem_id\", \"attempt\", \"score\"#\"outcome\", \"timestamp\"\n",
    ").order_by(\"timestamp\").filter(attempt=1).values_list('user_id', flat=True)\n",
    "\n",
    "data = UserLog.objects.filter(problem__in=problems).exclude(outcome='S').annotate(\n",
    "    attempt=Subquery(attempts, output_field=IntegerField())).annotate(\n",
    "    score=Case(\n",
    "        When(outcome='F', then=Value(0)),\n",
    "        When(outcome='P', then=Value(1)),\n",
    "        output_field=IntegerField())).values_list(\n",
    "    \"user__id\", \"problem_id\", \"attempt\", \"score\", #\"outcome\", #\"timestamp\"\n",
    ").order_by(\"timestamp\").filter(user__in=users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train set after 20% of students attempts to compare cold-start problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 10\n",
    "TRAIN_ATTEMPTS_TEST = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_idx(user_id):\n",
    "    return users_idx.index(user_id)\n",
    "\n",
    "def get_question_idx(question_id):\n",
    "    return questions_idx.index(question_id)\n",
    "\n",
    "map_user_id = np.vectorize(get_user_idx)    \n",
    "map_question_id = np.vectorize(get_question_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 328 ms, total: 12.7 s\n",
      "Wall time: 7min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_coldstart = []\n",
    "for i in range(N_RUNS):\n",
    "    # Open data\n",
    "    with open(\"%s/2020_06_08_run_%d.pkl\" % (folder, i), \"rb\") as pklfile:\n",
    "        dataset = pickle.load(pklfile)\n",
    "    test_users = []\n",
    "    # Remove test users and get max attempt\n",
    "    max_attempt = 0\n",
    "    for row in dataset['test_set']:\n",
    "        test_users.append(row[0])\n",
    "        if row[2] > max_attempt:\n",
    "            max_attempt = row[2]\n",
    "    train_users = set(range(len(dataset['users_idx']))) - set(test_users)\n",
    "    \n",
    "    # Create \"train set\" to test with students only after up to 20% of their attempts\n",
    "    # In the same way the test users were chosen\n",
    "    train_set = []\n",
    "    for idx, user in enumerate(train_users):\n",
    "        user_attempts = data.filter(user_id=dataset['users_idx'][user])\n",
    "        train_attempts = int(np.ceil(TRAIN_ATTEMPTS_TEST*user_attempts.count()))\n",
    "        train_set.extend(list(user_attempts.filter(attempt__gt=train_attempts).order_by('attempt')))\n",
    "        \n",
    "    # Mapping users and questions to be in 0-len index\n",
    "    users_idx = dataset['users_idx']\n",
    "    questions_idx = dataset['questions_idx']\n",
    "    train_set = np.asarray(train_set)\n",
    "    train_set[:,0] = map_user_id(train_set[:,0])\n",
    "    train_set[:,1] = map_question_id(train_set[:,1])\n",
    "    data_coldstart.append({'train_set': train_set, \n",
    "                 'users_idx': dataset['users_idx'], \n",
    "                 'questions_idx': dataset['questions_idx'],\n",
    "                 'attempts': max_attempt+1\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_coldstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_tensor(data, tensor, X, filter_attempt=False):\n",
    "    for item in data:\n",
    "        s_idx, q_idx, a_idx, outcome = item\n",
    "\n",
    "        # If attempt is over max value, ignore it\n",
    "        if filter_attempt and a_idx >= filter_attempt:\n",
    "            continue\n",
    "\n",
    "        tensor[s_idx, q_idx, a_idx] = outcome\n",
    "        X[s_idx, q_idx, a_idx] = 1\n",
    "        \n",
    "    tensor[np.where(X[:,:] == 0)] = None\n",
    "    return tensor, X\n",
    "    \n",
    "def transform_data(data):\n",
    "    N_STUDENTS = len(data['users_idx'])\n",
    "    N_QUESTIONS = len(data['questions_idx'])\n",
    "    train_set = data['train_set']\n",
    "\n",
    "    max_attempt = np.max(train_set[:,2])+1\n",
    "#     max_attempt = 20\n",
    "    student_performance_coldstart = np.zeros((N_STUDENTS, N_QUESTIONS, data['attempts']))\n",
    "    X_coldstart = np.zeros(student_performance_coldstart.shape)\n",
    "\n",
    "    student_performance_coldstart, X_coldstart = add_to_tensor(train_set, \n",
    "                                                               student_performance_coldstart, \n",
    "                                                               X_coldstart, \n",
    "                                                               filter_attempt=data['attempts']\n",
    "                                                              )\n",
    "    \n",
    "    return student_performance_coldstart, X_coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(user = settings.DATABASES[\"default\"][\"USER\"],\n",
    "                                  password = settings.DATABASES[\"default\"][\"PASSWORD\"],\n",
    "                                  host = settings.DATABASES[\"default\"][\"HOST\"],\n",
    "                                  port = settings.DATABASES[\"default\"][\"PORT\"],\n",
    "                                  database = settings.DATABASES[\"default\"][\"NAME\"])\n",
    "connection.autocommit=True\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.55 s, sys: 5.67 s, total: 10.2 s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"\"\"select dataset, experiment_id, sp, x, sp_hat, sk_hat, q_matrix_hat, mu, del_questions\n",
    "from EDM2020_2020_06_05 where method='fdtf' \n",
    "and attempts_train=150 and concepts >=11 and concepts <=16 and mu='0.1' and dataset <> 'run_all'\n",
    "order by experiment_id\"\"\"\n",
    "cursor.execute(query)\n",
    "row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2020_06_08_run_0\n",
      "Train: 1989 \n",
      "Train all: 3017\n",
      "Dataset 2020_06_08_run_0\n",
      "Train: 1989 \n",
      "Train all: 3017\n",
      "Dataset 2020_06_08_run_0\n",
      "Train: 1989 \n",
      "Train all: 3017\n",
      "Dataset 2020_06_08_run_0\n",
      "Train: 1989 \n",
      "Train all: 3017\n",
      "Dataset 2020_06_08_run_0\n",
      "Train: 1989 \n",
      "Train all: 3017\n",
      "Dataset 2020_06_08_run_1\n",
      "Train: 1935 \n",
      "Train all: 2900\n",
      "Dataset 2020_06_08_run_1\n",
      "Train: 1935 \n",
      "Train all: 2900\n",
      "Dataset 2020_06_08_run_1\n",
      "Train: 1935 \n",
      "Train all: 2900\n",
      "Dataset 2020_06_08_run_1\n",
      "Train: 1935 \n",
      "Train all: 2900\n",
      "Dataset 2020_06_08_run_1\n",
      "Train: 1935 \n",
      "Train all: 2900\n",
      "Dataset 2020_06_08_run_2\n",
      "Train: 2029 \n",
      "Train all: 2994\n",
      "Dataset 2020_06_08_run_2\n",
      "Train: 2029 \n",
      "Train all: 2994\n",
      "Dataset 2020_06_08_run_2\n",
      "Train: 2029 \n",
      "Train all: 2994\n",
      "Dataset 2020_06_08_run_2\n",
      "Train: 2029 \n",
      "Train all: 2994\n",
      "Dataset 2020_06_08_run_2\n",
      "Train: 2029 \n",
      "Train all: 2994\n",
      "Dataset 2020_06_08_run_3\n",
      "Train: 1750 \n",
      "Train all: 3023\n",
      "Dataset 2020_06_08_run_3\n",
      "Train: 1750 \n",
      "Train all: 3023\n",
      "Dataset 2020_06_08_run_3\n",
      "Train: 1750 \n",
      "Train all: 3023\n",
      "Dataset 2020_06_08_run_3\n",
      "Train: 1750 \n",
      "Train all: 3023\n",
      "Dataset 2020_06_08_run_3\n",
      "Train: 1750 \n",
      "Train all: 3023\n",
      "Dataset 2020_06_08_run_4\n",
      "Train: 1909 \n",
      "Train all: 2997\n",
      "Dataset 2020_06_08_run_4\n",
      "Train: 1909 \n",
      "Train all: 2997\n",
      "Dataset 2020_06_08_run_4\n",
      "Train: 1909 \n",
      "Train all: 2997\n",
      "Dataset 2020_06_08_run_4\n",
      "Train: 1909 \n",
      "Train all: 2997\n",
      "Dataset 2020_06_08_run_4\n",
      "Train: 1909 \n",
      "Train all: 2997\n",
      "Dataset 2020_06_08_run_5\n",
      "Train: 1891 \n",
      "Train all: 3122\n",
      "Dataset 2020_06_08_run_5\n",
      "Train: 1891 \n",
      "Train all: 3122\n",
      "Dataset 2020_06_08_run_5\n",
      "Train: 1891 \n",
      "Train all: 3122\n",
      "Dataset 2020_06_08_run_5\n",
      "Train: 1891 \n",
      "Train all: 3122\n",
      "Dataset 2020_06_08_run_5\n",
      "Train: 1891 \n",
      "Train all: 3122\n",
      "Dataset 2020_06_08_run_7\n",
      "Train: 1696 \n",
      "Train all: 3048\n",
      "Dataset 2020_06_08_run_7\n",
      "Train: 1696 \n",
      "Train all: 3048\n",
      "Dataset 2020_06_08_run_7\n",
      "Train: 1696 \n",
      "Train all: 3048\n",
      "Dataset 2020_06_08_run_7\n",
      "Train: 1696 \n",
      "Train all: 3048\n",
      "Dataset 2020_06_08_run_7\n",
      "Train: 1696 \n",
      "Train all: 3048\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "total = 0\n",
    "update_queries = []\n",
    "\n",
    "while row:\n",
    "    dataset, exp_id, sp, X, sp_hat, sk_hat, q_matrix_hat, mu, del_questions = row\n",
    "    \n",
    "    dataset_id = int(dataset.split('_')[-1])\n",
    "        \n",
    "    sp_coldstart, X_coldstart = transform_data(data_coldstart[dataset_id])\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    sp = np.asarray(sp)\n",
    "    sp_hat = np.asarray(sp_hat)\n",
    "    sk_hat = np.asarray(sk_hat)\n",
    "    q_matrix_hat = np.asarray(q_matrix_hat)\n",
    "    \n",
    "    # Delete unused questions\n",
    "    X_coldstart = np.delete(X_coldstart, del_questions, axis=1)\n",
    "    sp_coldstart = np.delete(sp_coldstart, del_questions, axis=1)\n",
    "    \n",
    "    # More attempts than train attempts\n",
    "    q_matrix_hat = np.asarray(q_matrix_hat)\n",
    "    sp_hat_coldstart = np.zeros((sp_hat.shape[0], sp_hat.shape[1], sp_coldstart.shape[2]))\n",
    "    attempts_begin = min(sp_coldstart.shape[2], sp_hat.shape[2])\n",
    "    \n",
    "    ### CHOOSE ATTEMPT \n",
    "    # Not seen attempts\n",
    "    attempts = X_coldstart.shape[2]\n",
    "    # First 20 attempts\n",
    "    #attempts = attempts_begin\n",
    "\n",
    "    sp_hat_coldstart[:, :, :attempts_begin] = sp_hat[:, :, :attempts_begin]\n",
    "    sk_hat_coldstart = np.zeros((sk_hat.shape[0], sk_hat.shape[1], sp_coldstart.shape[2]))\n",
    "    sk_hat_coldstart[:, :, :attempts_begin] = sk_hat[:, :, :attempts_begin]\n",
    "    \n",
    "    \n",
    "    # Calculate SK and SP for the next test attempts\n",
    "    for attempt in range(1, attempts):\n",
    "        students = np.where(X_coldstart[:,:, attempt] == 1)[0]\n",
    "#         print(\"%d students in attempt %d\" % (len(students), attempt))\n",
    "        for student in students:\n",
    "            sk_hat_coldstart[student, :, attempt] = (2*sk_hat_coldstart[student, :, attempt-1]) + \\\n",
    "                                            2*(1-sk_hat_coldstart[student, :, attempt-1])/(1+np.exp(\n",
    "                                                -mu*np.dot(X_coldstart[student, :, attempt], \n",
    "                                                           q_matrix_hat.T))) - 1\n",
    "            sp_hat_coldstart[student, :, attempt] = np.dot(sk_hat_coldstart[student, :, attempt], \n",
    "                                                           q_matrix_hat)\n",
    "    \n",
    "    # Get test predicted values\n",
    "    y = sp_coldstart[np.where(X_coldstart[:,:,:attempt+1] == 1)]\n",
    "    y_pred = sp_hat_coldstart[np.where(X_coldstart[:,:,:attempt+1] == 1)]\n",
    "    rmse = np.sqrt(np.mean(np.power(y - y_pred, 2)))\n",
    "    total += 1\n",
    "    \n",
    "    update = {\n",
    "#        \"train_rmse_cs_att\": rmse,\n",
    "         \"train_rmse_cs_all\": rmse,\n",
    "         \"max_train_cs_att\": attempts\n",
    "    }\n",
    "    \n",
    "    # Write PSQL query\n",
    "    update_query = \"UPDATE EDM2020_2020_06_05 SET \"\n",
    "    update_list = []\n",
    "    query_values = []\n",
    "    for key, value in update.items():\n",
    "        update_list.append(key + \"= %s\")\n",
    "        query_values.append(value)\n",
    "    \n",
    "    update_query += \", \".join(update_list) + \"where experiment_id = %s\"\n",
    "    query_values.append(exp_id)\n",
    "    query_values = tuple(query_values)\n",
    "    query = cursor.mogrify(update_query, query_values)\n",
    "    update_queries.append(query)\n",
    "    \n",
    "    print(\"Dataset %s\" % dataset)\n",
    "    print(\"Train: %d \" % y.shape[0])\n",
    "    print(\"Train all: %d\" % np.where(X == 1)[0].shape[0])\n",
    "        \n",
    "    try:\n",
    "        row = cursor.fetchone()\n",
    "    except psycopg2.ProgrammingError:\n",
    "        row = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 0 ns, total: 15.6 ms\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for query in update_queries:\n",
    "    cursor.execute(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
