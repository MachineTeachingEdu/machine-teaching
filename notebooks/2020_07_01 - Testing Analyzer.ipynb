{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import io\n",
    "import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load analyzer.py\n",
    "import io\n",
    "import tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_params = {\n",
    "    \"ngram_range\": (1,3)\n",
    "}\n",
    "\n",
    "removed_itens = ['NEWLINE', 'ENDMARKER', \"NL\", 'COMMENT', 'ERRORTOKEN']\n",
    "allowed_itens = ['NAME', 'OP', 'INDENT', 'DEDENT', 'STRING', 'NUMBER']\n",
    "\n",
    "def python_analyzer(doc):\n",
    "    words = []\n",
    "    not_found = []\n",
    "    #print(vectorizer_params)\n",
    "    vectorizer = CountVectorizer(**vectorizer_params)\n",
    "    file = io.StringIO(doc)\n",
    "    try:\n",
    "        for token in tokenize.generate_tokens(file.readline):\n",
    "            token_type = tokenize.tok_name[token[0]]\n",
    "\n",
    "            # Redundant conditional to make sure we're getting all the token types\n",
    "            if token_type not in removed_itens:\n",
    "                if token_type in allowed_itens:\n",
    "                    # If it's a variable or reserved name, keep it\n",
    "                    if token_type == \"NAME\":\n",
    "                        words.append(token[1])\n",
    "                    elif token_type == \"INDENT\":\n",
    "                        # Adding indent for all indentations\n",
    "                        words.append(\"is_indent\")\n",
    "                    elif token_type == \"DEDENT\":\n",
    "                        # Adding dedent for all indentations\n",
    "                        words.append(\"is_dedent\")\n",
    "                    elif token_type == \"STRING\":\n",
    "                        # Adding is_string for every string\n",
    "                        words.append(\"is_string\")\n",
    "                    elif token_type == \"NUMBER\":\n",
    "                        # Adding is_number for every number:\n",
    "                        words.append(\"is_number\")\n",
    "                    elif token_type == \"OP\":\n",
    "                        # If it's operator, then we'll divide in several types\n",
    "                        lookup = {\n",
    "                            \"+\": \"is_op_arit\",\n",
    "                            \"+=\": \"is_op_arit\",\n",
    "                            \"-\": \"is_op_arit\",\n",
    "                            \"*\": \"is_op_arit\",\n",
    "                            \"**\": \"is_op_arit\",\n",
    "                            \"/\": \"is_op_arit\",\n",
    "                            \"//\": \"is_op_arit\",\n",
    "                            \"%\": \"is_op_arit\",\n",
    "                            \">\": \"is_op_logic\",\n",
    "                            \"<\": \"is_op_logic\",\n",
    "                            \">=\": \"is_op_logic\",\n",
    "                            \"<=\": \"is_op_logic\",\n",
    "                            \"==\": \"is_op_logic\",\n",
    "                            \"-=\": \"is_op_logic\",\n",
    "                            \"!=\": \"is_op_logic\",\n",
    "                            \"[\": \"is_list\",\n",
    "    #                         \"]\": \"is_list\",\n",
    "                            \"{\": \"is_dict\",\n",
    "    #                         \"}\": \"is_dict\",\n",
    "                            \".\": \"is_class\",\n",
    "                            \"=\": \"is_attribution\",\n",
    "                            \":\": \"is_block\"\n",
    "                        }\n",
    "                        try:\n",
    "                            words.append(lookup[token[1]])\n",
    "                        except KeyError:\n",
    "                            not_found.append(token[1])\n",
    "    #     print(\"not found: %s\" % set(not_found))\n",
    "    except (IndentationError, tokenize.TokenError):\n",
    "        pass\n",
    "    return vectorizer._word_ngrams(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"def light(A, B):\n",
    "    if A == 1 and B == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer(analyzer = analyzer.python_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'light',\n",
       " 'A',\n",
       " 'B',\n",
       " 'is_block',\n",
       " 'is_indent',\n",
       " 'if',\n",
       " 'A',\n",
       " 'is_op_logic',\n",
       " 'is_number',\n",
       " 'and',\n",
       " 'B',\n",
       " 'is_op_logic',\n",
       " 'is_number',\n",
       " 'is_block',\n",
       " 'is_indent',\n",
       " 'return',\n",
       " 'True',\n",
       " 'is_dedent',\n",
       " 'else',\n",
       " 'is_block',\n",
       " 'is_indent',\n",
       " 'return',\n",
       " 'False',\n",
       " 'is_dedent',\n",
       " 'is_dedent',\n",
       " 'def light',\n",
       " 'light A',\n",
       " 'A B',\n",
       " 'B is_block',\n",
       " 'is_block is_indent',\n",
       " 'is_indent if',\n",
       " 'if A',\n",
       " 'A is_op_logic',\n",
       " 'is_op_logic is_number',\n",
       " 'is_number and',\n",
       " 'and B',\n",
       " 'B is_op_logic',\n",
       " 'is_op_logic is_number',\n",
       " 'is_number is_block',\n",
       " 'is_block is_indent',\n",
       " 'is_indent return',\n",
       " 'return True',\n",
       " 'True is_dedent',\n",
       " 'is_dedent else',\n",
       " 'else is_block',\n",
       " 'is_block is_indent',\n",
       " 'is_indent return',\n",
       " 'return False',\n",
       " 'False is_dedent',\n",
       " 'is_dedent is_dedent',\n",
       " 'def light A',\n",
       " 'light A B',\n",
       " 'A B is_block',\n",
       " 'B is_block is_indent',\n",
       " 'is_block is_indent if',\n",
       " 'is_indent if A',\n",
       " 'if A is_op_logic',\n",
       " 'A is_op_logic is_number',\n",
       " 'is_op_logic is_number and',\n",
       " 'is_number and B',\n",
       " 'and B is_op_logic',\n",
       " 'B is_op_logic is_number',\n",
       " 'is_op_logic is_number is_block',\n",
       " 'is_number is_block is_indent',\n",
       " 'is_block is_indent return',\n",
       " 'is_indent return True',\n",
       " 'return True is_dedent',\n",
       " 'True is_dedent else',\n",
       " 'is_dedent else is_block',\n",
       " 'else is_block is_indent',\n",
       " 'is_block is_indent return',\n",
       " 'is_indent return False',\n",
       " 'return False is_dedent',\n",
       " 'False is_dedent is_dedent']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.analyzer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=1 (NAME), string='def', start=(1, 0), end=(1, 3), line='def light(A, B):\\n')\n",
      "TokenInfo(type=1 (NAME), string='light', start=(1, 4), end=(1, 9), line='def light(A, B):\\n')\n",
      "TokenInfo(type=53 (OP), string='(', start=(1, 9), end=(1, 10), line='def light(A, B):\\n')\n",
      "TokenInfo(type=1 (NAME), string='A', start=(1, 10), end=(1, 11), line='def light(A, B):\\n')\n",
      "TokenInfo(type=53 (OP), string=',', start=(1, 11), end=(1, 12), line='def light(A, B):\\n')\n",
      "TokenInfo(type=1 (NAME), string='B', start=(1, 13), end=(1, 14), line='def light(A, B):\\n')\n",
      "TokenInfo(type=53 (OP), string=')', start=(1, 14), end=(1, 15), line='def light(A, B):\\n')\n",
      "TokenInfo(type=53 (OP), string=':', start=(1, 15), end=(1, 16), line='def light(A, B):\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 16), end=(1, 17), line='def light(A, B):\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(2, 0), end=(2, 4), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=1 (NAME), string='if', start=(2, 4), end=(2, 6), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=1 (NAME), string='A', start=(2, 7), end=(2, 8), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=53 (OP), string='==', start=(2, 9), end=(2, 11), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(2, 12), end=(2, 13), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=1 (NAME), string='and', start=(2, 14), end=(2, 17), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=1 (NAME), string='B', start=(2, 18), end=(2, 19), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=53 (OP), string='==', start=(2, 20), end=(2, 22), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(2, 23), end=(2, 24), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=53 (OP), string=':', start=(2, 24), end=(2, 25), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 25), end=(2, 26), line='    if A == 1 and B == 1:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(3, 0), end=(3, 8), line='        return True\\n')\n",
      "TokenInfo(type=1 (NAME), string='return', start=(3, 8), end=(3, 14), line='        return True\\n')\n",
      "TokenInfo(type=1 (NAME), string='True', start=(3, 15), end=(3, 19), line='        return True\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(3, 19), end=(3, 20), line='        return True\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(4, 4), end=(4, 4), line='    else:\\n')\n",
      "TokenInfo(type=1 (NAME), string='else', start=(4, 4), end=(4, 8), line='    else:\\n')\n",
      "TokenInfo(type=53 (OP), string=':', start=(4, 8), end=(4, 9), line='    else:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 9), end=(4, 10), line='    else:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(5, 0), end=(5, 8), line='        return False')\n",
      "TokenInfo(type=1 (NAME), string='return', start=(5, 8), end=(5, 14), line='        return False')\n",
      "TokenInfo(type=1 (NAME), string='False', start=(5, 15), end=(5, 20), line='        return False')\n",
      "TokenInfo(type=4 (NEWLINE), string='', start=(5, 20), end=(5, 21), line='')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(6, 0), end=(6, 0), line='')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(6, 0), end=(6, 0), line='')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(6, 0), end=(6, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "file = io.StringIO(text)\n",
    "for t in tokenize.generate_tokens(file.readline):\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
