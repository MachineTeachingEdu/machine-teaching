{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import io\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#DB\n",
    "from questions.models import Solution\n",
    "\n",
    "# Preprocessing\n",
    "import tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions_obj = Solution.objects.filter(ignore=False).order_by('id')\n",
    "solutions_obj.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 54 documents\n",
      "Success in parsing all documents! You may go on!\n"
     ]
    }
   ],
   "source": [
    "removed_itens = ['NEWLINE', 'STRING', 'ENDMARKER', 'NUMBER', 'INDENT', 'DEDENT', \"NL\", 'COMMENT', 'ERRORTOKEN']\n",
    "allowed_itens = ['NAME', 'OP']\n",
    "# cursor = db.conn.cursor()\n",
    "docs = []\n",
    "docs_id = []\n",
    "# doc_category = []\n",
    "errors = []\n",
    "questions = []\n",
    "solutions = []\n",
    "\n",
    "# lendo os dados\n",
    "# cursor.execute(\"\"\"\n",
    "# SELECT solution.id, solution.content, problem.content, problem.category FROM solution, problem where solution.problem_id = problem.id;\n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "for sol in solutions_obj:\n",
    "    file = io.StringIO(sol.content)\n",
    "    doc = []\n",
    "    try:\n",
    "        for item in tokenize.generate_tokens(file.readline):\n",
    "            if tokenize.tok_name[item[0]] not in removed_itens:\n",
    "                if tokenize.tok_name[item[0]] in allowed_itens:\n",
    "                    doc.append(item[1])\n",
    "                else:\n",
    "                    print(\"%s %s\" % (tokenize.tok_name[item[0]], item[1]))\n",
    "    except (IndentationError, tokenize.TokenError):\n",
    "        errors.append(\"Please, fix solution %d before continuing\" % (sol.id))\n",
    "    \n",
    "    if doc == []:\n",
    "        continue\n",
    "    docs.append(' '.join(doc))\n",
    "    docs_id.append(sol.id)\n",
    "#     doc_category.append(row[-1])\n",
    "    questions.append(sol.problem.content)\n",
    "    solutions.append(sol.content)\n",
    "\n",
    "print(\"Got %d documents\" %(solutions_obj.count()))\n",
    "\n",
    "if not errors:\n",
    "    print(\"Success in parsing all documents! You may go on!\")\n",
    "else:\n",
    "    for item in errors:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "#                              stop_words = ['print'],   \\\n",
    "                             #max_features = 26d,\n",
    "                             binary=False,\n",
    "                             min_df=0.1\n",
    "                            ) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "# Document-term matrix\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows containing only zeros (weird exercises)\n",
    "solution_sample = train_data_features[~(train_data_features==0).all(1)]\n",
    "docs_id = np.asarray(docs_id)[~(train_data_features==0).all(1)]\n",
    "# doc_category = np.asarray(doc_category)[~(train_data_features==0).all(1)]\n",
    "questions = np.asarray(questions)[~(train_data_features==0).all(1)]\n",
    "docs_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 54)\n"
     ]
    }
   ],
   "source": [
    "# Create W as a similarity matrix\n",
    "W = cosine_similarity(solution_sample) # nS x nS\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('similarity.pkl', 'wb') as pklfile:\n",
    "    pickle.dump(W, pklfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
