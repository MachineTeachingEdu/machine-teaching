{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB\n",
    "import os\n",
    "\n",
    "# Helpers\n",
    "import numpy as np\n",
    "# import qgrid\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from analyzer import python_analyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from vectorizer import NCutVectorizer\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    base_path = \"bbc\"\n",
    "    with open(\"%s/bbc.terms\" % base_path, \"r\") as txtfile:\n",
    "        vocab = txtfile.read().split('\\n')\n",
    "    return vocab\n",
    "    \n",
    "def get_docs(topics):\n",
    "    docs = []\n",
    "    base_path = \"bbc\"\n",
    "    for t in topics:\n",
    "        docs_title = os.listdir(\"%s/%s\" % (base_path,t))\n",
    "        for item in docs_title:\n",
    "            with open('%s/%s/%s' %(base_path, t, item), 'r') as txtfile:\n",
    "                try:\n",
    "                    docs.append(txtfile.read())\n",
    "                except UnicodeDecodeError:\n",
    "                    print(\"Error on doc %s/%s\" % (t,item))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load vectorizer.py\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from analyzer import python_analyzer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NCutVectorizer(object):\n",
    "    def __init__(self, analyzer, binary, min_df, vocabulary):\n",
    "        self.vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                                          binary=binary,\n",
    "                                          min_df=min_df,\n",
    "                                          vocabulary=vocabulary)\n",
    "\n",
    "    def fit_transform(self, docs):\n",
    "        train_data_features = self.vectorizer.fit_transform(docs)\n",
    "        train_data_features = train_data_features.toarray()\n",
    "\n",
    "        # Calculate NCut-weight\n",
    "        doc_mat_norm = normalize(train_data_features)\n",
    "        S = np.dot(doc_mat_norm.T, doc_mat_norm) + 0.001\n",
    "        D = np.power(np.sum(S, axis=1), -0.5) * np.eye(S.shape[0])\n",
    "        Y = np.dot(D, train_data_features.T)\n",
    "        return Y.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing solutions into bag of words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(docs, vectorizer_method, binary=False, min_df=0.2):\n",
    "    vocab = get_vocab()\n",
    "#     vectorizer = vectorizer_method(analyzer = python_analyzer,\n",
    "    vectorizer = vectorizer_method(analyzer = 'word',\n",
    "                                   binary=binary,\n",
    "                                   min_df=min_df,\n",
    "                                   vocabulary=vocab) \n",
    "    train_data_features = vectorizer.fit_transform(docs)\n",
    "    try:\n",
    "        train_data_features = train_data_features.toarray()\n",
    "    # It's already an array\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clusters(*args):\n",
    "    # Import libraries\n",
    "    # DB\n",
    "    import psycopg2\n",
    "    # Helpers\n",
    "    import base64\n",
    "    import pickle\n",
    "    import time\n",
    "    import numpy as np\n",
    "    # Learning\n",
    "    from clustering import Clustering\n",
    "    # Evaluation\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "    from gap import Gap\n",
    "    from coherence import calculate_umass_coherence\n",
    "    \n",
    "    # Get arguments\n",
    "    dataset, v, m, b, train_data_features, k, dist, method = args\n",
    "    \n",
    "    # Instanciate objects\n",
    "    clustering = Clustering(train_data_features, k, metric=dist)\n",
    "    gap = Gap(train_data_features, k, nrefs=20, distance=dist)\n",
    "    \n",
    "    # Cluster\n",
    "    start = time.time()\n",
    "    model, document_topic, word_topic = getattr(clustering, method)()\n",
    "    clusters = document_topic.argmax(axis=1)\n",
    "    end = time.time()\n",
    "    clustering_time = end-start\n",
    "\n",
    "    # Compute Gap\n",
    "    start = time.time()\n",
    "    k_gap = gap.calculate_gap(clustering, method)\n",
    "#     time.sleep(10)\n",
    "#     k_gap = [3,2]\n",
    "    end = time.time()\n",
    "    gap_time = end-start\n",
    "\n",
    "    # Compute silhouette. Keep single to values to be able to plot it later\n",
    "    start = time.time()\n",
    "    k_silhouette = silhouette_score(train_data_features, clusters, metric=dist)\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(train_data_features, clusters, metric=dist)\n",
    "    end = time.time()\n",
    "    silhouette_time = end-start\n",
    "\n",
    "    # UMass coherence\n",
    "    start = time.time()\n",
    "    k_coherence = calculate_umass_coherence(train_data_features, word_topic, clusters, k)\n",
    "    end = time.time()\n",
    "    coherence_time = end-start\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": dataset,\n",
    "        \"X\": train_data_features,\n",
    "        \"y\": clusters,\n",
    "        \"vectorizer\": v.__name__,\n",
    "        \"is_binary\": b,\n",
    "        \"min_df\": m,\n",
    "        \"distance\": dist,\n",
    "        \"k\": k,\n",
    "        \"method\": method,\n",
    "        \"model\": base64.b64encode(pickle.dumps(model)),\n",
    "        \"clustering_time\": clustering_time,\n",
    "        \"gap\": k_gap[0],\n",
    "        \"gap_std\": k_gap[1],\n",
    "        \"gap_time\": gap_time,\n",
    "        \"silhouette\": k_silhouette,\n",
    "        \"silhouette_samples\": sample_silhouette_values,\n",
    "        \"silhouette_time\": silhouette_time,\n",
    "        \"coherence_samples\": k_coherence[0],\n",
    "        \"coherence_med\": k_coherence[1],\n",
    "        \"coherence_std\": k_coherence[2],\n",
    "        \"coherence_time\": coherence_time,\n",
    "        \"coherence_k\": len(k_coherence[0]),\n",
    "    }\n",
    "    \n",
    "    # Connect to DB\n",
    "    connection = psycopg2.connect(user = \"machineteaching\",\n",
    "                                  password = \"\",\n",
    "                                  host = \"localhost\",\n",
    "#                                   port = \"5432\",\n",
    "                                  database = \"machineteaching\")\n",
    "    connection.autocommit=True\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Write PSQL query\n",
    "    insert_query_base = \"INSERT INTO EXPERIMENTS \"\n",
    "    column_value = []\n",
    "    insert_format = []\n",
    "    query_values = []\n",
    "    for col in row.keys():\n",
    "        if isinstance(row[col], np.ndarray):\n",
    "            query_values.append(row[col].tolist())\n",
    "        else:\n",
    "            query_values.append(row[col])\n",
    "        column_value.append(col)\n",
    "        insert_format.append(\"%s\")\n",
    "\n",
    "    insert_query = insert_query_base + \"(\" + \", \".join(column_value) + \") VALUES \"\n",
    "    insert_query += \"(\" + \", \".join(insert_format) + \")\"\n",
    "    query_values = tuple(query_values)\n",
    "    query = cursor.mogrify(insert_query, query_values)\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7272fe3a7cc44bc9b80c5bf7151ad094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='docs', max=4, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on doc sport/199.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cd17d6ed684592bd1dd09c4573b726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='vectorizer', max=3, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fc998a7a6344049d465d7cdcd77c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='min_df', max=9, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c7cbdc4a404c31a468e8d094b54754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='binary', max=2, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbbc5445d334220b7d630dc577dd040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='clusters', max=46, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd472c74dd24c018caf65b32d30725e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='metric', max=3, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f99b845b9345e3bb1f9ebac912af93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='method', max=1, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OperationalError",
     "evalue": "SSL SYSCALL error: EOF detected\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3ee960bc1ec8>\u001b[0m in \u001b[0;36mrun_clusters\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mquery_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsert_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: SSL SYSCALL error: EOF detected\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers = [\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer, \n",
    "    NCutVectorizer\n",
    "]\n",
    "min_df = np.arange(0.05, 0.5, 0.05)\n",
    "# min_df = [0.05]\n",
    "binary = [\n",
    "    True, \n",
    "    False\n",
    "]\n",
    "cluster_methods = ['nmf']#, 'lda']\n",
    "#, 'hierarchical', 'gaussian_mixture', 'spectral_clustering']\n",
    "metric = [\n",
    "    'euclidean', \n",
    "    'cosine', \n",
    "    'correlation'\n",
    "]\n",
    "\n",
    "topics = {}\n",
    "topics[\"bbc5_vocab\"] = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "topics[\"bbc4_vocab\"] = [\"business\", \"politics\", \"sport\", \"tech\"]\n",
    "topics[\"bbc3_vocab\"] = [\"politics\", \"sport\", \"tech\"]\n",
    "topics[\"bbc2_vocab\"] = [\"politics\", \"tech\"]\n",
    "\n",
    "# Testing how the metrics work for several group of topics\n",
    "for dataset, topic_list in tqdm_notebook(topics.items(), desc=\"docs\"):\n",
    "    docs = get_docs(topic_list)\n",
    "    \n",
    "    # Grid search\n",
    "    for v in tqdm_notebook(vectorizers, desc=\"vectorizer\", leave=False):\n",
    "        for m in tqdm_notebook(min_df, desc=\"min_df\", leave=False):\n",
    "            \n",
    "            for b in tqdm_notebook(binary, desc=\"binary\", leave=False):\n",
    "                train_data_features = create_bag_of_words(docs, v, binary=b, min_df=m)\n",
    "\n",
    "                # Remove rows containing only zeros (weird exercises)\n",
    "                solution_sample = train_data_features[~(train_data_features==0).all(1)]\n",
    "                if solution_sample.shape != train_data_features.shape:\n",
    "                    error = {\n",
    "                        \"vectorizer\": v,\n",
    "                        \"min_df\": m,\n",
    "                        \"binary\": b\n",
    "                    }\n",
    "                    print(\"ERROR: %s\" % error)\n",
    "\n",
    "                clusters = range(2, int(np.sqrt(min(train_data_features.shape)))+1)\n",
    "                for k in tqdm_notebook(clusters, desc=\"clusters\", leave=False):\n",
    "                    for dist in tqdm_notebook(metric, desc=\"metric\", leave=False):\n",
    "                        for method in tqdm_notebook(cluster_methods, desc=\"method\", leave=False):\n",
    "                            # Sequences to be sent to map function\n",
    "                            args = [dataset, v, m, b, train_data_features, \n",
    "                                    k, dist, method]\n",
    "                            run_clusters(*args)\n",
    "#                             break\n",
    "#                         break\n",
    "#                     break\n",
    "#                 break\n",
    "#             result = lbview.map_async(run_clusters, *args)\n",
    "#             start = time.time()\n",
    "#             jobs = 0\n",
    "#             N = len(result)\n",
    "#             while(not result.ready()):\n",
    "#                 while result.progress == jobs:\n",
    "#                     time.sleep(1)\n",
    "#                     elapsed = time.time()\n",
    "#                     print('\\r', '%d/%d tasks finished after %d s' % (result.progress, N, (elapsed-start)), end='')\n",
    "#                 os.system(\"echo %d/%d tasks finished after %d s >> log.txt\" % (result.progress, N, (elapsed-start)))\n",
    "#                 jobs += 1\n",
    "#             speedup = \"Speedup: %.2f x\" % (1.0 * result.serial_time / result.wall_time)\n",
    "#             os.system(\"echo %s >> log.txt\" % speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14580"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
